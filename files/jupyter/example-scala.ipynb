{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Scala Example**\n",
    "\n",
    "This notebook will show you how to use **Apache Spark** with **Scala** to perform a simple word count.\n",
    "\n",
    "You can run a cell by pressing **\"shift-enter\"**, which will compute the current cell and advance to the next cell, or by clicking in a cell and pressing **\"control-enter\"**, which will compute the current cell and remain in that cell.\n",
    "\n",
    "** This notebook covers: **\n",
    "* *Part 1:* Required Libraries\n",
    "* *Part 2:* Spark Context\n",
    "* *Part 3:* Word Count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "This section shows how to import the required libraries.\n",
    "\n",
    "Extra libraries can also be imported from maven repositories. See the comment bellow or the kernel [documentation](https://github.com/alexarchambault/jupyter-scala/blob/master/README.md) to know more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// If the spark jars where not on the Worker classpath they could be added,\n",
    "// directly from the maven repository, using the following code:\n",
    "\n",
    "//classpath.add(\"org.apache.spark\" % \"spark-core_2.11\" % \"2.0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkConf;\n",
    "import org.apache.spark.SparkContext;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Context\n",
    "\n",
    "This section shows how to initialize and configure a basic SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val conf = new SparkConf()\n",
    "    //.setMaster(\"spark://localhost:7077\")\n",
    "    .setMaster(\"local[2]\")\n",
    "    .setAppName(\"Word Count Scala App\")\n",
    "val sc = new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count\n",
    "\n",
    "This section shows the Word Counter application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// load a text file\n",
    "val textFile = sc.textFile(\"/srv/spark/LICENSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// count the times each word appears on the file\n",
    "val counts = textFile.flatMap(line => line.split(\" \"))\n",
    "                 .map(word => (word, 1))\n",
    "                 .reduceByKey(_ + _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// top 15 words\n",
    "counts.takeOrdered(15)(Ordering[Int].reverse.on(x=>x._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*notebook writen by [fscm](https://github.com/fscm)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
